{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Configuration du proxy ScraperAPI\n",
    "SCRAPER_API_KEY = \"13711e6869a8f4bdf18432db22cf05ae\"  # Remplacez par votre clé API ScraperAPI\n",
    "proxies = {\n",
    "    \"https\": f\"http://scraperapi:{SCRAPER_API_KEY}@proxy-server.scraperapi.com:8001\"\n",
    "}\n",
    "\n",
    "# Scraper les informations d'un restaurant\n",
    "def scrape_restaurant(restaurant_url):\n",
    "    try:\n",
    "        # Faire une requête HTTP avec le proxy ScraperAPI\n",
    "        response = requests.get(restaurant_url, proxies=proxies, verify=False)\n",
    "        \n",
    "        # Si la requête est réussie\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extraire les informations du restaurant\n",
    "            nom = soup.find('h1', class_='rRtyp').text.strip() if soup.find('h1', class_='rRtyp') else \"Nom non trouvé\"\n",
    "            \n",
    "            localisation = soup.find('div', class_='biGQs _P pZUbB hmDzD').text.strip() if soup.find('div', class_='biGQs _P pZUbB hmDzD') else \"Localisation non trouvée\"\n",
    "            categorie = \"Restaurant\"\n",
    "            tags = soup.find('span', class_='VdWAl').text.strip() if soup.find('span', class_='VdWAl') else \"Tags non trouvés\"\n",
    "            # Chercher la note globale dans un <div> ou un <span>\n",
    "            note_globale = None\n",
    "            note_div = soup.find('div', class_='biGQs _P fiohW hzzSG uuBRH')\n",
    "            note_span = soup.find('span', class_='biGQs _P fiohW uuBRH')\n",
    "            \n",
    "            if note_div:\n",
    "                note_globale = note_div.text.strip()\n",
    "            elif note_span:\n",
    "                note_globale = note_span.text.strip()\n",
    "\n",
    "            # Convertir la note en float, en remplaçant la virgule par un point si nécessaire\n",
    "            if note_globale:\n",
    "                note_globale = float(note_globale.replace(\",\", \".\"))\n",
    "            else:\n",
    "                note_globale = 0.0\n",
    "\n",
    "\n",
    "            # Retourner les informations sous forme de dictionnaire\n",
    "            return {\n",
    "                \n",
    "                \"nom\": nom,\n",
    "                \"localisation\": localisation,\n",
    "                \"categorie\": categorie,\n",
    "                \"tags\": tags,\n",
    "                \"note_globale\": note_globale,\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Erreur lors de la récupération de la page {restaurant_url}, code de statut {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du scraping du restaurant {restaurant_url} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Enregistrer dans un fichier CSV\n",
    "def save_to_csv(data, filename=\"restaurants.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Données enregistrées dans {filename}\")\n",
    "\n",
    "# Script principal\n",
    "def main():\n",
    "    # Liste des URLs des restaurants à scraper\n",
    "    restaurant_urls = [\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d7171160-Reviews-KUMA_cantine-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d1330943-Reviews-Mattsam_Restaurant_Messob-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d7698838-Reviews-Brasserie_des_Confluences-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d715010-Reviews-Christian_Tetedoie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d4338972-Reviews-Creperie_La_Marie_Morgane-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d15130370-Reviews-Fiston_Bouchon_Lyonnais-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d3727154-Reviews-Les_Terrasses_de_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d9597301-Reviews-Kenbo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n",
    "    ]\n",
    "\n",
    "    all_restaurants = []  # Liste pour stocker les informations des restaurants\n",
    "\n",
    "    try:\n",
    "        restaurant_id = 1  # Initialiser le compteur\n",
    "        for url in restaurant_urls:\n",
    "            print(f\"Scraping des informations pour : {url}\")\n",
    "            restaurant_data = scrape_restaurant(url)\n",
    "            if restaurant_data:\n",
    "                restaurant_data[\"id_restaurant\"] = restaurant_id  # Ajouter l'ID\n",
    "                all_restaurants.append(restaurant_data)\n",
    "                restaurant_id += 1\n",
    "                time.sleep(random.uniform(10, 30))  # Pause aléatoire entre les requêtes\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur générale : {e}\")\n",
    "\n",
    "    # Sauvegarder les données dans un fichier CSV\n",
    "    if all_restaurants:\n",
    "        save_to_csv(all_restaurants)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d7171160-Reviews-KUMA_cantine-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soulm\\OneDrive\\Documents\\GitHub\\Friands\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d1330943-Reviews-Mattsam_Restaurant_Messob-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soulm\\OneDrive\\Documents\\GitHub\\Friands\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d7698838-Reviews-Brasserie_des_Confluences-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soulm\\OneDrive\\Documents\\GitHub\\Friands\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d715010-Reviews-Christian_Tetedoie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soulm\\OneDrive\\Documents\\GitHub\\Friands\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soulm\\OneDrive\\Documents\\GitHub\\Friands\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d4338972-Reviews-Creperie_La_Marie_Morgane-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soulm\\OneDrive\\Documents\\GitHub\\Friands\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d15130370-Reviews-Fiston_Bouchon_Lyonnais-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soulm\\OneDrive\\Documents\\GitHub\\Friands\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d3727154-Reviews-Les_Terrasses_de_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soulm\\OneDrive\\Documents\\GitHub\\Friands\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d19896976-Reviews-Empanadas_Club-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soulm\\OneDrive\\Documents\\GitHub\\Friands\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping des informations pour : https://www.tripadvisor.fr/Restaurant_Review-g187265-d9597301-Reviews-Kenbo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\soulm\\OneDrive\\Documents\\GitHub\\Friands\\env\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'proxy-server.scraperapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données enregistrées dans restaurants.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Configuration du proxy ScraperAPI\n",
    "SCRAPER_API_KEY = \"13711e6869a8f4bdf18432db22cf05ae\"  # Remplacez par votre clé API ScraperAPI\n",
    "proxies = {\n",
    "    \"https\": f\"http://scraperapi:{SCRAPER_API_KEY}@proxy-server.scraperapi.com:8001\"\n",
    "}\n",
    "\n",
    "# Scraper les informations d'un restaurant\n",
    "def scrape_restaurant(restaurant_url):\n",
    "    try:\n",
    "        # Faire une requête HTTP avec le proxy ScraperAPI\n",
    "        response = requests.get(restaurant_url, proxies=proxies, verify=False)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extraire les informations du restaurant\n",
    "            nom = soup.find('h1', class_='rRtyp').text.strip() if soup.find('h1', class_='rRtyp') else \"Nom non trouvé\"\n",
    "            localisation = soup.find('div', class_='OFtgC').text.strip() if soup.find('div', class_='OFtgC') else \"Localisation non trouvée\"\n",
    "            categorie = \"Restaurant\"\n",
    "            # Trouver les tags en utilisant une expression régulière pour la classe\n",
    "            tags = soup.find('span', class_=re.compile(r'(VdWAl|HUMGB cPbcf)'))\n",
    "            tags = tags.text.strip() if tags else \"Tags non trouvés\"\n",
    "            # Note globale\n",
    "            note_globale = soup.find('div', class_='biGQs _P fiohW hzzSG uuBRH')\n",
    "            note_globale = float(note_globale.text.strip().replace(\",\", \".\")) if note_globale else 0.0\n",
    "            \n",
    "            # Nombre total de commentaires\n",
    "            total_comments = soup.find('span', class_='GPKsO').text.strip() if soup.find('span', class_='GPKsO') else \"0\" \n",
    "            \n",
    "            \n",
    "            # Retourner les informations sous forme de dictionnaire\n",
    "            return {\n",
    "                \"id_restaurant\": None,  # Placeholder pour l'ID\n",
    "                \"nom\": nom,\n",
    "                \"localisation\": localisation,\n",
    "                \"categorie\": categorie,\n",
    "                \"tags\": tags,\n",
    "                \"note_globale\": note_globale,\n",
    "                \"total_comments\": total_comments,\n",
    "                \"url\": restaurant_url\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Erreur lors de la récupération de la page {restaurant_url}, code de statut {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du scraping du restaurant {restaurant_url} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Enregistrer dans un fichier CSV\n",
    "def save_to_csv(data, filename=\"restaurants.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    # Réorganiser les colonnes pour placer `id_restaurant` en premier\n",
    "    columns_order = [\"id_restaurant\",  \"nom\", \"localisation\", \"categorie\", \"tags\", \"note_globale\", \"total_comments\", \"url\"]\n",
    "    df = df[columns_order]\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "    print(f\"Données enregistrées dans {filename}\")\n",
    "\n",
    "# Script principal\n",
    "def main():\n",
    "    # Liste des URLs des restaurants à scraper\n",
    "    restaurant_urls = [\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d7171160-Reviews-KUMA_cantine-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d1330943-Reviews-Mattsam_Restaurant_Messob-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d7698838-Reviews-Brasserie_des_Confluences-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d715010-Reviews-Christian_Tetedoie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d4338972-Reviews-Creperie_La_Marie_Morgane-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d15130370-Reviews-Fiston_Bouchon_Lyonnais-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d3727154-Reviews-Les_Terrasses_de_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d19896976-Reviews-Empanadas_Club-Lyon_Rhone_Auvergne_Rhone_Alpes.html\",\n",
    "        \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d9597301-Reviews-Kenbo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    all_restaurants = []  # Liste pour stocker les informations des restaurants\n",
    "\n",
    "    try:\n",
    "        restaurant_id = 1  # Initialiser le compteur\n",
    "        for url in restaurant_urls:\n",
    "            print(f\"Scraping des informations pour : {url}\")\n",
    "            restaurant_data = scrape_restaurant(url)\n",
    "            if restaurant_data:\n",
    "                restaurant_data[\"id_restaurant\"] = restaurant_id  # Ajouter l'ID\n",
    "                all_restaurants.append(restaurant_data)\n",
    "                restaurant_id += 1\n",
    "                time.sleep(random.uniform(10, 30))  # Pause aléatoire entre les requêtes\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur générale : {e}\")\n",
    "\n",
    "    # Sauvegarder les données dans un fichier CSV\n",
    "    if all_restaurants:\n",
    "        save_to_csv(all_restaurants)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## script pour scrapper avis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration du proxy ScraperAPI\n",
    "SCRAPER_API_KEY = \"13711e6869a8f4bdf18432db22cf05ae\"  # Remplacez par votre clé API ScraperAPI\n",
    "proxies = {\n",
    "    \"https\": f\"http://scraperapi:{SCRAPER_API_KEY}@proxy-server.scraperapi.com:8001\"\n",
    "}\n",
    "\n",
    "# Fonction pour scraper les avis d'un restaurant\n",
    "def scrape_avis(restaurant_url, id_restaurant):\n",
    "    avis_list = []  # Liste pour stocker les avis\n",
    "    page_num = 0  # Numéro de page des avis\n",
    "    avis_id = 1  # Initialiser l'ID des avis\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            url = f\"{restaurant_url}-or{page_num * 15}\"  # URL de la page des avis\n",
    "            print(f\"Scraping des avis pour le restaurant {id_restaurant}, page {page_num + 1}: {url}\")\n",
    "\n",
    "            # Requête HTTP\n",
    "            response = requests.get(url, proxies=proxies, verify=False)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Erreur lors de la récupération de la page {url}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Sélectionner les conteneurs d'avis\n",
    "            avis_containers = soup.find_all('div', class_='_c')  # Ajuster la classe si nécessaire\n",
    "\n",
    "            if not avis_containers:  # Si aucun avis trouvé, on arrête\n",
    "                break\n",
    "\n",
    "            for avis in avis_containers:\n",
    "                # Nom de l'utilisateur\n",
    "                nom_utilisateur = avis.find('span', class_='biGQs _P fiohW fOtGX').text.strip() if avis.find('span', class_='biGQs _P fiohW fOtGX') else \"Anonyme\"\n",
    "\n",
    "                # Note du restaurant\n",
    "                svg_element = avis.find('svg', class_='UctUV')\n",
    "                note_restaurant = None\n",
    "                if svg_element:\n",
    "                    title_element = svg_element.find('title')\n",
    "                    if title_element:\n",
    "                        note_text = title_element.text.strip()\n",
    "                        note_restaurant = float(note_text.split(' ')[0].replace(',', '.'))\n",
    "\n",
    "                # Date de l'avis\n",
    "                date_avis = avis.find('div', class_='biGQs _P pZUbB ncFvv osNWb').text.strip() if avis.find('div', class_='biGQs _P pZUbB ncFvv osNWb') else None\n",
    "\n",
    "                # Titre avis\n",
    "                titre_avis = avis.find('div', class_=\"biGQs _P fiohW qWPrE ncFvv fOtGX\").text.strip() if avis.find('div', class_=\"biGQs _P fiohW qWPrE ncFvv fOtGX\") else \"titre non disponible\"\n",
    "\n",
    "                # Contenu de l'avis\n",
    "                contenu_avis = avis.find('span', class_='JguWG').text.strip() if avis.find('span', class_='JguWG') else \"Contenu non disponible\"\n",
    "\n",
    "                # Ajouter les informations à la liste\n",
    "                avis_list.append({\n",
    "                    \"id_avis\": avis_id,\n",
    "                    \"id_restaurant\": id_restaurant,\n",
    "                    \"nom_utilisateur\": nom_utilisateur,\n",
    "                    \"note_restaurant\": note_restaurant,\n",
    "                    \"date_avis\": date_avis,\n",
    "                    \"titre_avis\": titre_avis,\n",
    "                    \"contenu_avis\": contenu_avis,\n",
    "                })\n",
    "                avis_id += 1\n",
    "\n",
    "            # Pause aléatoire pour éviter le blocage\n",
    "            time.sleep(random.uniform(5, 15))\n",
    "\n",
    "            # Passer à la page suivante\n",
    "            page_num += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du scraping des avis : {e}\")\n",
    "            break\n",
    "\n",
    "    return avis_list\n",
    "\n",
    "# Fonction pour sauvegarder les avis dans un fichier CSV existant\n",
    "def save_avis_to_csv(data, filename=\"avis.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Vérifier si le fichier existe déjà\n",
    "    if os.path.isfile(filename):\n",
    "        # Ajouter les nouvelles données sans écrire l'en-tête\n",
    "        df.to_csv(filename, mode='a', index=False, header=False, encoding='utf-8')\n",
    "    else:\n",
    "        # Créer le fichier et écrire l'en-tête\n",
    "        df.to_csv(filename, mode='w', index=False, header=True, encoding='utf-8')\n",
    "\n",
    "    print(f\"Données des avis enregistrées dans {filename}\")\n",
    "\n",
    "# Script principal\n",
    "def main():\n",
    "    # Liste des restaurants avec leurs URLs et IDs\n",
    "    restaurants = [\n",
    "        {\"id_restaurant\": 2, \"url\": \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d1330943-Reviews-Mattsam_Restaurant_Messob-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"},\n",
    "        {\"id_restaurant\": 3, \"url\": \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d7698838-Reviews-Brasserie_des_Confluences-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"},\n",
    "        {\"id_restaurant\": 4, \"url\": \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d715010-Reviews-Christian_Tetedoie-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"},\n",
    "        {\"id_restaurant\": 5, \"url\": \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"},\n",
    "        {\"id_restaurant\": 6, \"url\": \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d4338972-Reviews-Creperie_La_Marie_Morgane-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"},\n",
    "        {\"id_restaurant\": 7, \"url\": \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d15130370-Reviews-Fiston_Bouchon_Lyonnais-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"},\n",
    "        {\"id_restaurant\": 8, \"url\": \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d3727154-Reviews-Les_Terrasses_de_Lyon-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"},\n",
    "        {\"id_restaurant\": 9, \"url\": \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d6694423-Reviews-Gang_Nam-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"},\n",
    "        {\"id_restaurant\": 10, \"url\": \"https://www.tripadvisor.fr/Restaurant_Review-g187265-d9597301-Reviews-Kenbo-Lyon_Rhone_Auvergne_Rhone_Alpes.html\"}\n",
    "    ]\n",
    "\n",
    "    all_avis = []  # Liste pour tous les avis\n",
    "\n",
    "    # Scraper les avis pour chaque restaurant\n",
    "    for restaurant in restaurants:\n",
    "        try:\n",
    "            avis = scrape_avis(restaurant[\"url\"], restaurant[\"id_restaurant\"])\n",
    "            save_avis_to_csv(avis)  # Sauvegarde incrémentale après chaque restaurant\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur pour le restaurant {restaurant['id_restaurant']} : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration du proxy ScraperAPI\n",
    "SCRAPER_API_KEY = \"13711e6869a8f4bdf18432db22cf05ae\"  # Remplacez par votre clé API ScraperAPI\n",
    "proxies = {\n",
    "    \"https\": f\"http://scraperapi:{SCRAPER_API_KEY}@proxy-server.scraperapi.com:8001\"\n",
    "}\n",
    "\n",
    "\n",
    "# Fonction pour scraper les avis d'un restaurant\n",
    "def scrape_avis(restaurant_url, id_restaurant):\n",
    "    avis_list = []  # Liste pour stocker les avis\n",
    "    page_num = 0  # Numéro de page des avis\n",
    "    avis_id = 1  # Initialiser l'ID des avis\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            url = f\"{restaurant_url}-or{page_num * 15}\"  # URL de la page des avis\n",
    "            print(f\"Scraping des avis pour le restaurant {id_restaurant}, page {page_num + 1}: {url}\")\n",
    "\n",
    "            # Réessayer jusqu'à 3 fois en cas d'erreur\n",
    "            max_retries = 5\n",
    "            retries = 0\n",
    "            while retries < max_retries:\n",
    "                response = requests.get(url, proxies=proxies, verify=False)\n",
    "                if response.status_code == 200:\n",
    "                    break  # Succès, sortir de la boucle de réessai\n",
    "                print(f\"Erreur lors de la récupération de la page {url}, tentative {retries + 1}\")\n",
    "                retries += 1\n",
    "                time.sleep(random.uniform(2, 5))  # Pause avant de réessayer\n",
    "\n",
    "            if retries == max_retries:\n",
    "                print(f\"Échec après {max_retries} tentatives pour la page {url}. Passer à la page suivante.\")\n",
    "                page_num += 1\n",
    "                continue  # Passer à la page suivante\n",
    "\n",
    "            # Analyse HTML\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Sélectionner les conteneurs d'avis\n",
    "            avis_containers = soup.find_all('div', class_='_c')  # Ajuster la classe si nécessaire\n",
    "\n",
    "            if not avis_containers:  # Si aucun avis trouvé, on arrête\n",
    "                print(f\"Aucun avis trouvé sur la page {url}. Fin du scraping pour ce restaurant.\")\n",
    "                break\n",
    "\n",
    "            for avis in avis_containers:\n",
    "                # Nom de l'utilisateur\n",
    "                nom_utilisateur = avis.find('span', class_='biGQs _P fiohW fOtGX').text.strip() if avis.find('span', class_='biGQs _P fiohW fOtGX') else \"Anonyme\"\n",
    "\n",
    "                # Note du restaurant\n",
    "                svg_element = avis.find('svg', class_='UctUV')\n",
    "                note_restaurant = None\n",
    "                if svg_element:\n",
    "                    title_element = svg_element.find('title')\n",
    "                    if title_element:\n",
    "                        note_text = title_element.text.strip()\n",
    "                        note_restaurant = float(note_text.split(' ')[0].replace(',', '.'))\n",
    "\n",
    "                # Date de l'avis\n",
    "                date_avis = avis.find('div', class_='biGQs _P pZUbB ncFvv osNWb').text.strip() if avis.find('div', class_='biGQs _P pZUbB ncFvv osNWb') else None\n",
    "\n",
    "                # Titre avis\n",
    "                titre_avis = avis.find('div', class_=\"biGQs _P fiohW qWPrE ncFvv fOtGX\").text.strip() if avis.find('div', class_=\"biGQs _P fiohW qWPrE ncFvv fOtGX\") else \"titre non disponible\"\n",
    "\n",
    "                # Contenu de l'avis\n",
    "                contenu_avis = avis.find('span', class_='JguWG').text.strip() if avis.find('span', class_='JguWG') else \"Contenu non disponible\"\n",
    "\n",
    "                # Ajouter les informations à la liste\n",
    "                avis_list.append({\n",
    "                    \"id_avis\": avis_id,\n",
    "                    \"id_restaurant\": id_restaurant,\n",
    "                    \"nom_utilisateur\": nom_utilisateur,\n",
    "                    \"note_restaurant\": note_restaurant,\n",
    "                    \"date_avis\": date_avis,\n",
    "                    \"titre_avis\": titre_avis,\n",
    "                    \"contenu_avis\": contenu_avis,\n",
    "                })\n",
    "                avis_id += 1\n",
    "\n",
    "            # Pause aléatoire pour éviter le blocage\n",
    "            time.sleep(random.uniform(5, 15))\n",
    "\n",
    "            # Passer à la page suivante\n",
    "            page_num += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du scraping des avis pour la page {url} : {e}\")\n",
    "            break  # Arrêter le scraping pour ce restaurant en cas d'erreur grave\n",
    "\n",
    "    return avis_list\n",
    "\n",
    "\n",
    "# Fonction pour sauvegarder les avis dans un fichier CSV existant\n",
    "def save_avis_to_csv(data, filename=\"avis.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Vérifier si le fichier existe déjà\n",
    "    if os.path.isfile(filename):\n",
    "        # Ajouter les nouvelles données sans écrire l'en-tête\n",
    "        df.to_csv(filename, mode='a', index=False, header=False, encoding='utf-8')\n",
    "    else:\n",
    "        # Créer le fichier et écrire l'en-tête\n",
    "        df.to_csv(filename, mode='w', index=False, header=True, encoding='utf-8')\n",
    "\n",
    "    print(f\"Données des avis enregistrées dans {filename}\")\n",
    "\n",
    "# Fonction pour lire les restaurants depuis un fichier CSV\n",
    "def read_restaurants(file_path=\"restaurants.csv\"):\n",
    "    try:\n",
    "        restaurants_df = pd.read_csv(file_path)\n",
    "        return restaurants_df\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la lecture du fichier {file_path} : {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction principale pour scraper les avis d'un seul restaurant\n",
    "def scrape_single_restaurant(restaurant):\n",
    "    id_restaurant = restaurant['id_restaurant']\n",
    "    restaurant_url = restaurant['url']\n",
    "\n",
    "    print(f\"Début du scraping pour le restaurant ID: {id_restaurant}\")\n",
    "    avis = scrape_avis(restaurant_url, id_restaurant)\n",
    "    save_avis_to_csv(avis)\n",
    "\n",
    "# Exemple d'utilisation\n",
    "def main():\n",
    "    # Charger le fichier des restaurants\n",
    "    restaurants = read_restaurants()\n",
    "    if restaurants is None:\n",
    "        print(\"Aucun restaurant chargé. Fin du script.\")\n",
    "        return\n",
    "\n",
    "    # Scraper les avis pour les restaurants de l'indice 4 à 9\n",
    "    for i in range(7, 8):  # De 4 à 9 inclus\n",
    "        restaurant = restaurants.iloc[i].to_dict()  # Convertir chaque ligne en dictionnaire\n",
    "        try:\n",
    "            scrape_single_restaurant(restaurant)  # Scrapper les avis pour ce restaurant\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du scraping pour le restaurant {restaurant['id_restaurant']} : {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scrap longitude et latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 93 Rue Masséna, 69006 Lyon France...\n",
      "Processing: 85 rue Massena, 69006 Lyon France...\n",
      "Processing: 86 Quai Perrache, 69002 Lyon France...\n",
      "Processing: 4 Rue du Professeur Pierre Marion, 69005 Lyon France...\n",
      "Processing: 6 rue Thomassin, 69002 Lyon France...\n",
      "Processing: 23 Rue de la Charité, 69002 Lyon France...\n",
      "Processing: 33 Rue Saint-Jean, 69005 Lyon France...\n",
      "Processing: 25 Montee Saint Barthelemy, 69005 Lyon France...\n",
      "Processing: 4 Rue Pizay, 69001 Lyon France...\n",
      "Processing: 3 avenue Berthelot, 69007 Lyon France...\n",
      "Données enrichies sauvegardées dans restaurants_enriched.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def get_coordinates(address):\n",
    "    \"\"\"Utilise l'API Nominatim pour obtenir latitude et longitude à partir d'une adresse.\"\"\"\n",
    "    url = \"https://nominatim.openstreetmap.org/search\"\n",
    "    params = {\n",
    "        \"q\": address,\n",
    "        \"format\": \"json\",\n",
    "        \"addressdetails\": 1,\n",
    "        \"limit\": 1,\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"YourAppName/1.0 (your@email.com)\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data:\n",
    "            lat = data[0].get(\"lat\")\n",
    "            lon = data[0].get(\"lon\")\n",
    "            return lat, lon\n",
    "        else:\n",
    "            print(f\"Aucune donnée trouvée pour l'adresse : {address}\")\n",
    "    else:\n",
    "        print(f\"Erreur lors de la requête : {response.status_code}\")\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_density_of_restaurants(lat, lon, radius=500):\n",
    "    \"\"\"Utilise l'API Overpass pour récupérer les restaurants à proximité d'une latitude/longitude.\"\"\"\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json];\n",
    "    (\n",
    "      node[\"amenity\"=\"restaurant\"](around:{radius},{lat},{lon});\n",
    "      way[\"amenity\"=\"restaurant\"](around:{radius},{lat},{lon});\n",
    "      relation[\"amenity\"=\"restaurant\"](around:{radius},{lat},{lon});\n",
    "    );\n",
    "    out body;\n",
    "    \"\"\"\n",
    "    response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Le nombre total de restaurants trouvés\n",
    "        return len(data[\"elements\"])\n",
    "    else:\n",
    "        print(f\"Erreur lors de la requête Overpass : {response.status_code}\")\n",
    "        return 0\n",
    "    \n",
    "def get_transport_info(lat, lon, radius=500):\n",
    "    \"\"\"Récupère les informations sur les transports publics à proximité d'une latitude/longitude.\"\"\"\n",
    "    overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json];\n",
    "    (\n",
    "      node[\"highway\"=\"bus_stop\"](around:{radius},{lat},{lon});\n",
    "      node[\"railway\"=\"station\"](around:{radius},{lat},{lon});\n",
    "      node[\"amenity\"=\"subway\"](around:{radius},{lat},{lon});\n",
    "    );\n",
    "    out body;\n",
    "    \"\"\"\n",
    "    response = requests.get(overpass_url, params={'data': overpass_query})\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Nombre d'éléments de transport trouvés\n",
    "        return len(data[\"elements\"])\n",
    "    else:\n",
    "        print(f\"Erreur lors de la requête Overpass : {response.status_code}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def enrich_restaurant_data(input_csv, output_csv):\n",
    "    \"\"\"Ajoute les coordonnées géographiques, les transports et la densité de restaurants au fichier CSV des restaurants.\"\"\"\n",
    "    restaurants = pd.read_csv(input_csv)\n",
    "    latitudes, longitudes, densities, transport_counts = [], [], [], []\n",
    "\n",
    "    for _, row in restaurants.iterrows():\n",
    "        print(f\"Processing: {row['localisation']}...\")\n",
    "        latitude, longitude = get_coordinates(row['localisation'])\n",
    "        \n",
    "        # Ajouter les coordonnées récupérées\n",
    "        latitudes.append(latitude)\n",
    "        longitudes.append(longitude)\n",
    "        \n",
    "        # Récupérer la densité des restaurants autour du restaurant\n",
    "        if latitude and longitude:\n",
    "            density = get_density_of_restaurants(latitude, longitude)\n",
    "            transport_count = get_transport_info(latitude, longitude)\n",
    "            densities.append(density)\n",
    "            transport_counts.append(transport_count)\n",
    "        else:\n",
    "            densities.append(None)\n",
    "            transport_counts.append(None)\n",
    "        \n",
    "        time.sleep(1)  # Respecter les limites de l'API\n",
    "\n",
    "    # Ajouter les nouvelles colonnes latitude, longitude, densité et transport au dataframe\n",
    "    restaurants['latitude'] = latitudes\n",
    "    restaurants['longitude'] = longitudes\n",
    "    restaurants['restaurant_density'] = densities\n",
    "    restaurants['transport_count'] = transport_counts\n",
    "\n",
    "    # Sauvegarder le fichier CSV enrichi avec les nouvelles informations\n",
    "    restaurants.to_csv(output_csv, index=False)\n",
    "    print(f\"Données enrichies sauvegardées dans {output_csv}\")\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "enrich_restaurant_data(\"restaurants.csv\", \"restaurants_enriched.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
