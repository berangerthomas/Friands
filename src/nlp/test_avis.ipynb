{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install spacy nltk scikit-learn wordcloud factor_analyzer\\npython -m spacy download fr_core_news_md\\npython -m nltk.downloader vader_lexicon stopwords\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pip install spacy nltk scikit-learn wordcloud factor_analyzer\n",
    "python -m spacy download fr_core_news_md\n",
    "python -m nltk.downloader vader_lexicon stopwords\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import spacy\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from pathlib import Path\n",
    "from sqlutils import sqlutils\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewAnalyzer:\n",
    "    def __init__(self, lang: str = \"fr\"):\n",
    "        \"\"\"\n",
    "        Initialise l'analyseur d'avis avec les modèles et ressources nécessaires.\n",
    "\n",
    "        Args:\n",
    "            lang: Code de langue ('fr' ou 'en')\n",
    "        \"\"\"\n",
    "        # Charger les modèles et ressources\n",
    "        self.lang = lang\n",
    "        self.nlp = spacy.load(\"fr_core_news_md\" if lang == \"fr\" else \"en_core_web_md\")\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        self.stop_words = set(stopwords.words(\"french\" if lang == \"fr\" else \"english\"))\n",
    "        self.reviews = []\n",
    "        self.processed_reviews = []\n",
    "\n",
    "    def add_review(\n",
    "        self,\n",
    "        review: str,\n",
    "        rating: Optional[float] = None,\n",
    "        metadata: Optional[Dict] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Ajoute un avis à analyser avec métadonnées optionnelles.\n",
    "\n",
    "        Args:\n",
    "            review: Texte de l'avis\n",
    "            rating: Note numérique (optionnelle)\n",
    "            metadata: Dictionnaire de métadonnées (date, utilisateur, etc.)\n",
    "        \"\"\"\n",
    "        self.reviews.append(\n",
    "            {\"text\": review, \"rating\": rating, \"metadata\": metadata or {}}\n",
    "        )\n",
    "\n",
    "    def preprocess_text(self, text: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Prétraite le texte avec Spacy pour une analyse approfondie.\n",
    "\n",
    "        Returns:\n",
    "            Tuple contenant le texte nettoyé et les lemmes\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        # Extraire les tokens pertinents\n",
    "        tokens = [\n",
    "            token.lemma_.lower()\n",
    "            for token in doc\n",
    "            if not token.is_stop\n",
    "            and not token.is_punct\n",
    "            and not token.is_space\n",
    "            and len(token.text) > 2\n",
    "        ]\n",
    "\n",
    "        # Reconstruire le texte nettoyé\n",
    "        clean_text = \" \".join(tokens)\n",
    "\n",
    "        return clean_text, tokens\n",
    "\n",
    "    def analyze_sentiment(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Analyse le sentiment avec NLTK's VADER.\n",
    "\n",
    "        Returns:\n",
    "            Dictionnaire des scores de sentiment\n",
    "        \"\"\"\n",
    "        scores = self.sia.polarity_scores(text)\n",
    "        return {\n",
    "            \"positif\": scores[\"pos\"],\n",
    "            \"negatif\": scores[\"neg\"],\n",
    "            \"neutre\": scores[\"neu\"],\n",
    "            \"compose\": scores[\"compound\"],\n",
    "        }\n",
    "\n",
    "    def extract_topics(self, texts: List[str], n_topics: int = 3) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Extrait les principaux thèmes avec TF-IDF et K-means.\n",
    "\n",
    "        Returns:\n",
    "            Liste des mots-clés par thème\n",
    "        \"\"\"\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            max_features=1000, stop_words=list(self.stop_words)\n",
    "        )\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "        # Clustering pour identifier les thèmes\n",
    "        kmeans = KMeans(n_clusters=n_topics, random_state=42)\n",
    "        kmeans.fit(tfidf_matrix)\n",
    "\n",
    "        # Extraire les mots-clés par thème\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        topics = []\n",
    "\n",
    "        for cluster_center in kmeans.cluster_centers_:\n",
    "            top_indices = cluster_center.argsort()[-10:][::-1]\n",
    "            topics.append([feature_names[i] for i in top_indices])\n",
    "\n",
    "        return topics\n",
    "\n",
    "    def create_wordcloud(self, texts: List[str]) -> WordCloud:\n",
    "        \"\"\"\n",
    "        Génère un nuage de mots à partir des avis.\n",
    "\n",
    "        Returns:\n",
    "            Objet WordCloud\n",
    "        \"\"\"\n",
    "        combined_text = \" \".join(texts)\n",
    "        wordcloud = WordCloud(\n",
    "            width=800,\n",
    "            height=400,\n",
    "            background_color=\"white\",\n",
    "            stopwords=self.stop_words,\n",
    "            max_words=100,\n",
    "        ).generate(combined_text)\n",
    "\n",
    "        return wordcloud\n",
    "\n",
    "    def perform_factor_analysis(self, features: np.ndarray, n_factors: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Réalise une analyse factorielle des caractéristiques textuelles.\n",
    "\n",
    "        Returns:\n",
    "            Résultats de l'analyse factorielle\n",
    "        \"\"\"\n",
    "        fa = FactorAnalyzer(rotation=None, n_factors=n_factors)\n",
    "        fa.fit(features)\n",
    "\n",
    "        # Extraire les loadings et la variance expliquée\n",
    "        loadings = fa.loadings_\n",
    "        variance = fa.get_factor_variance()\n",
    "\n",
    "        return {\"loadings\": loadings, \"variance_expliquee\": variance}\n",
    "\n",
    "    def analyze_all(self, detailed: bool = False) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyse complète des avis avec visualisations optionnelles.\n",
    "\n",
    "        Args:\n",
    "            detailed: Si True, inclut des analyses supplémentaires\n",
    "\n",
    "        Returns:\n",
    "            Dictionnaire contenant toutes les analyses\n",
    "        \"\"\"\n",
    "        if not self.reviews:\n",
    "            return {\"error\": \"Aucun avis à analyser\"}\n",
    "\n",
    "        # Prétraitement\n",
    "        processed_texts = []\n",
    "        all_tokens = []\n",
    "        sentiments = []\n",
    "        ratings = []\n",
    "\n",
    "        for review in self.reviews:\n",
    "            clean_text, tokens = self.preprocess_text(review[\"text\"])\n",
    "            processed_texts.append(clean_text)\n",
    "            all_tokens.extend(tokens)\n",
    "\n",
    "            sentiment = self.analyze_sentiment(review[\"text\"])\n",
    "            sentiments.append(sentiment)\n",
    "\n",
    "            if review[\"rating\"]:\n",
    "                ratings.append(review[\"rating\"])\n",
    "\n",
    "        # Analyses de base\n",
    "        summary = {\n",
    "            \"nombre_avis\": len(self.reviews),\n",
    "            \"sentiments\": {\n",
    "                \"moyen\": np.mean([s[\"compose\"] for s in sentiments]),\n",
    "                \"distribution\": {\n",
    "                    \"positif\": np.mean([s[\"positif\"] for s in sentiments]),\n",
    "                    \"negatif\": np.mean([s[\"negatif\"] for s in sentiments]),\n",
    "                    \"neutre\": np.mean([s[\"neutre\"] for s in sentiments]),\n",
    "                },\n",
    "            },\n",
    "            \"note_moyenne\": np.mean(ratings) if ratings else None,\n",
    "            \"mots_frequents\": Counter(all_tokens).most_common(10),\n",
    "        }\n",
    "\n",
    "        if detailed:\n",
    "            # Analyses avancées\n",
    "            topics = self.extract_topics(processed_texts)\n",
    "            summary.update(\n",
    "                {\"themes\": topics, \"wordcloud\": self.create_wordcloud(processed_texts)}\n",
    "            )\n",
    "\n",
    "            # Analyses temporelles si les métadonnées contiennent des dates\n",
    "            dates = [\n",
    "                r[\"metadata\"].get(\"date\")\n",
    "                for r in self.reviews\n",
    "                if \"date\" in r[\"metadata\"]\n",
    "            ]\n",
    "            if dates:\n",
    "                summary[\"evolution_temporelle\"] = {\n",
    "                    \"dates\": dates,\n",
    "                    \"sentiments\": [s[\"compose\"] for s in sentiments],\n",
    "                }\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def generate_report(self, summary: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Génère un rapport textuel à partir des analyses.\n",
    "\n",
    "        Returns:\n",
    "            Rapport formaté en texte\n",
    "        \"\"\"\n",
    "        report = [\n",
    "            \"Rapport d'analyse des avis\",\n",
    "            \"========================\\n\",\n",
    "            f\"Nombre total d'avis analysés : {summary['nombre_avis']}\",\n",
    "            (\n",
    "                f\"Note moyenne : {summary['note_moyenne']:.1f}/5\"\n",
    "                if summary[\"note_moyenne\"]\n",
    "                else \"Pas de notes\"\n",
    "            ),\n",
    "            \"\\nAnalyse des sentiments:\",\n",
    "            f\"- Score moyen : {summary['sentiments']['moyen']:.2f}\",\n",
    "            f\"- Distribution : {summary['sentiments']['distribution']}\\n\",\n",
    "            \"\\nPrincipaux mots-clés:\",\n",
    "        ]\n",
    "\n",
    "        for mot, freq in summary[\"mots_frequents\"]:\n",
    "            report.append(f\"- {mot}: {freq} occurrences\")\n",
    "\n",
    "        if \"themes\" in summary:\n",
    "            report.extend(\n",
    "                [\n",
    "                    \"\\nThèmes principaux identifiés:\",\n",
    "                    *[\n",
    "                        f\"Thème {i+1}: {', '.join(theme[:5])}\"\n",
    "                        for i, theme in enumerate(summary[\"themes\"])\n",
    "                    ],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return \"\\n\".join(report)\n",
    "\n",
    "    def plot_sentiment_trends(self, summary: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Génère des visualisations des tendances de sentiment.\n",
    "        \"\"\"\n",
    "        if \"evolution_temporelle\" in summary:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.plot(\n",
    "                summary[\"evolution_temporelle\"][\"dates\"],\n",
    "                summary[\"evolution_temporelle\"][\"sentiments\"],\n",
    "            )\n",
    "            plt.title(\"Évolution des sentiments dans le temps\")\n",
    "            plt.xlabel(\"Date\")\n",
    "            plt.ylabel(\"Score de sentiment\")\n",
    "            plt.grid(True)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des dépendances requises :\n",
    "# pip install spacy nltk scikit-learn wordcloud factor_analyzer\n",
    "# python -m spacy download fr_core_news_md\n",
    "# python -m nltk.downloader vader_lexicon stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = ReviewAnalyzer(lang=\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chemin de la base de données: D:\\GitHub\\Friands\\data\\friands.db\n"
     ]
    }
   ],
   "source": [
    "# Obtenir le chemin du notebook\n",
    "notebook_path = get_ipython().starting_dir\n",
    "\n",
    "# Définir le lien vers la base de données en commencant par reculer de deux dossiers\n",
    "db_path = Path(notebook_path).parents[1] / \"data\" / \"friands.db\"\n",
    "\n",
    "# Vérifier le chemin de la base de données\n",
    "print(f\"Chemin de la base de données: {db_path.resolve()}\")\n",
    "\n",
    "# Créer une instance de sqlUtils\n",
    "db = sqlutils(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les avis par restaurants\n",
    "success, avis = db.select(\"SELECT id_restaurant, contenu_avis, note_restaurant, date_avis, nom_utilisateur FROM avis;\")\n",
    "\n",
    "for idres, avis in avis:\n",
    "    analyzer.add_review(avis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ajouter des avis avec métadonnées\n",
    "# analyzer.add_review(\n",
    "#     \"Le produit est excellent, très satisfait de mon achat !\",\n",
    "#     rating=5,\n",
    "#     metadata={\"date\": \"2024-01-01\", \"user_id\": \"user1\"},\n",
    "# )\n",
    "# analyzer.add_review(\n",
    "#     \"Livraison rapide mais qualité moyenne\",\n",
    "#     rating=3,\n",
    "#     metadata={\"date\": \"2024-01-02\", \"user_id\": \"user2\"},\n",
    "# )\n",
    "# analyzer.add_review(\n",
    "#     \"Je ne recommande pas, trop cher pour la qualité\",\n",
    "#     rating=2,\n",
    "#     metadata={\"date\": \"2024-01-03\", \"user_id\": \"user3\"},\n",
    "# )\n",
    "\n",
    "# Analyser et générer le rapport\n",
    "resultats = analyzer.analyze_all(detailed=True)\n",
    "rapport = analyzer.generate_report(resultats)\n",
    "print(rapport)\n",
    "\n",
    "# Afficher le nuage de mots\n",
    "if \"wordcloud\" in resultats:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(resultats[\"wordcloud\"])\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Nuage de mots des avis\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
